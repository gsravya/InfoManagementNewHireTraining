{"paragraphs":[{"title":"Get the Building Dataset and Location Dataset","text":"%sh\n\n#Get the Building Dataset\ncurl -O https://raw.githubusercontent.com/sblack4/spark-streaming-ml-demo/master/data/building.csv\nhadoop fs -mkdir -p /user/zeppelin/BigDataMLDemo\nhadoop fs -copyFromLocal -f building.csv /user/zeppelin/BigDataMLDemo/\ncurl -O https://raw.githubusercontent.com/sblack4/spark-streaming-ml-demo/master/data/Lat_Long_BuildingID.csv\nhadoop fs -copyFromLocal -f Lat_Long_BuildingID.csv /user/zeppelin/BigDataMLDemo/\nhadoop fs -chmod 777 /user/zeppelin/BigDataMLDemo/Lat_Long_BuildingID.csv\nhadoop fs -chmod 777 /user/zeppelin/BigDataMLDemo/building.csv\nhadoop fs -ls /user/zeppelin/BigDataMLDemo/","user":"anonymous","dateUpdated":"2018-03-15T18:34:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":"false"},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0 3395k    0 31923    0     0  36740      0  0:01:34 --:--:--  0:01:34 86983\r100 3395k  100 3395k    0     0  2639k      0  0:00:01  0:00:01 --:--:-- 4325k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r104  2397  104  2397    0     0   4935      0 --:--:-- --:--:-- --:--:-- 12818\nFound 2 items\n-rwxrwxrwx   2 zeppelin hdfs       2397 2018-03-12 23:36 /user/zeppelin/BigDataMLDemo/Lat_Long_BuildingID.csv\n-rwxrwxrwx   2 zeppelin hdfs    3476701 2018-03-12 23:36 /user/zeppelin/BigDataMLDemo/building.csv\n"}]},"apps":[],"jobName":"paragraph_1520891368750_-507682001","id":"20180312-214928_1778817307","dateCreated":"2018-03-12T21:49:28+0000","dateStarted":"2018-03-12T23:36:15+0000","dateFinished":"2018-03-12T23:36:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:182"},{"title":"Make sure the files are available in Big Data Cloud","text":"%sh\nhadoop fs -ls /user/zeppelin/BigDataMLDemo/","user":"anonymous","dateUpdated":"2018-03-23T20:08:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":"false"},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 2 items\n-rwxrwxrwx   2 zeppelin hdfs       2397 2018-03-12 23:36 /user/zeppelin/BigDataMLDemo/Lat_Long_BuildingID.csv\n-rwxrwxrwx   2 zeppelin hdfs    3476701 2018-03-12 23:36 /user/zeppelin/BigDataMLDemo/building.csv\n"}]},"apps":[],"jobName":"paragraph_1520900058041_-499775847","id":"20180313-001418_776478636","dateCreated":"2018-03-13T00:14:18+0000","dateStarted":"2018-03-13T00:14:38+0000","dateFinished":"2018-03-13T00:14:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:183"},{"title":"Cleanse the Data and get the final dataset in Big Data Cloud","text":"%sh\n\ncurl -O https://raw.githubusercontent.com/sblack4/spark-streaming-ml-demo/master/data/final_dataset.csv\nhadoop fs -copyFromLocal -f final_dataset.csv /user/zeppelin/BigDataMLDemo/\nhadoop fs -chmod 777 /user/zeppelin/BigDataMLDemo/final_dataset.csv\nhadoop fs -ls /user/zeppelin/BigDataMLDemo/final_dataset.csv","user":"anonymous","dateUpdated":"2018-03-15T18:34:23+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":"false"},"editorMode":"ace/mode/sh","editorHide":false,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r 75 2697k   75 2033k    0     0  1516k      0  0:00:01  0:00:01 --:--:-- 2682k\r100 2697k  100 2697k    0     0  1829k      0  0:00:01  0:00:01 --:--:-- 3023k\n-rwxrwxrwx   2 zeppelin hdfs    2762028 2018-03-13 01:50 /user/zeppelin/BigDataMLDemo/final_dataset.csv\n"}]},"apps":[],"jobName":"paragraph_1520905696017_-1552479789","id":"20180313-014816_1831716674","dateCreated":"2018-03-13T01:48:16+0000","dateStarted":"2018-03-13T01:50:13+0000","dateFinished":"2018-03-13T01:50:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:184"},{"title":"Load the dataset into SparkSQL Dataframe","text":"%pyspark\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark import sql\n\nsqlContext=sql.SQLContext(sc)\n\ndf=spark.read.format('csv').load(\"hdfs:///user/zeppelin/BigDataMLDemo/final_dataset.csv\")\ndf.show(10)\n\n\n","user":"anonymous","dateUpdated":"2018-03-14T18:34:16+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","editorHide":false,"tableHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 346, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 339, in <module>\n    exec(code)\n  File \"<stdin>\", line 4, in <module>\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/readwriter.py\", line 149, in load\n    return self._df(self._jreader.load(path))\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 65, in deco\n    s = e.java_exception.toString()\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1131, in __call__\n    answer = self.gateway_client.send_command(command)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response = connection.send_command(command)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/u01/bdcsce/usr/local/lib/python2.7/socket.py\", line 451, in readline\n    data = self._sock.recv(self._rbufsize)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 249, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"}]},"apps":[],"jobName":"paragraph_1520900286066_1202089487","id":"20180313-001806_725810989","dateCreated":"2018-03-13T00:18:06+0000","dateStarted":"2018-03-13T01:54:06+0000","dateFinished":"2018-03-13T02:09:36+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"title":"Save the temporary dataframe into permanent Hive Table and Query the Hive Table to display the content","text":"%pyspark\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark import sql\nfrom pyspark.sql import HiveContext\n\nhivecontext=SQLContext(sc)\ndf.write.saveAsTable('final_dataset')\nresult=hivecontext.sql(\"SELECT * FROM final_dataset\")\nresult.show(10)","user":"anonymous","dateUpdated":"2018-03-14T18:35:27+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520909002400_305587020","id":"20180313-024322_1267264771","dateCreated":"2018-03-13T02:43:22+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"title":"TRAINING MACHINE LEARNING MODEL: Utilizing past 60 days data to learn the model","text":"%pyspark\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import HashingTF, Tokenizer\nfrom pyspark.sql import Row, SQLContext\n\nimport os\nimport sys\n#import numpy\nfrom pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nfrom pyspark.mllib.classification import LogisticRegressionWithSGD\nfrom pyspark.mllib.regression import LabeledPoint\n\nLabeledDocument = Row(\"RecordID\", \"SystemInfo\", \"label\")\n\n# Define a function that parses the raw CSV file and returns an object of type LabeledDocument\n\ndef parseDocument(line):\n    values = [str(x) for x in line.split(',')]\n    if ((float(values[5]) < float(values[6])) and (float(values[2]>20))):\n        cold = 1.0\n    else:\n        cold = 0.0\n    textValue = str(values[1]) + \" \" + str(values[2])\n    return LabeledDocument((values[0]), textValue, cold)\n\n\ndata = sc.textFile(\"hdfs:///user/zeppelin/BigDataMLDemo/final_dataset.csv\")\nheader = data.first()\ndocuments = data.filter(lambda line: line != header).map(parseDocument)\n\n#documents = data.filter(lambda s: ).map(parseDocument)\n\ntraining = documents.toDF()\ntraining.show()\n\n\n","user":"anonymous","dateUpdated":"2018-03-16T19:45:39+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","tableHide":true,"editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 346, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 334, in <module>\n    exec(code)\n  File \"<stdin>\", line 23, in <module>\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1360, in first\n    rs = self.take(1)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1342, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 978, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 67, in deco\n    e.java_exception.getStackTrace()))\n  File \"/u01/bdcsce/usr/local/lib/python2.7/_abcoll.py\", line 605, in __iter__\n    v = self[i]\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 191, in __getitem__\n    return self.__compute_item(key)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 173, in __compute_item\n    answer = self._gateway_client.send_command(command)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response = connection.send_command(command)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/u01/bdcsce/usr/local/lib/python2.7/socket.py\", line 451, in readline\n    data = self._sock.recv(self._rbufsize)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 249, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"}]},"apps":[],"jobName":"paragraph_1520908728266_1754553023","id":"20180313-023848_971034231","dateCreated":"2018-03-13T02:38:48+0000","dateStarted":"2018-03-14T18:23:14+0000","dateFinished":"2018-03-14T18:23:53+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"title":"Store the training data into Hive Tables for Data Transformation","text":"%pyspark\n\nfrom pyspark.sql import Hivecontext\nhivecontext=SQLContext(sC)\ntraining.write.saveAsTable('training_building_data')\nresult.show(10)","user":"anonymous","dateUpdated":"2018-03-15T16:03:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true,"lineNumbers":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520912042343_-2010309152","id":"20180313-033402_1749247071","dateCreated":"2018-03-13T03:34:02+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"title":"Performing Data Transformation","text":"%pyspark\n\n\ntokenizer=Tokenizer(inputCol=\"SystemInfo\", outputCol=\"NewSystemInfo\")\n\nhashingTF=HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n\nlr= LogisticRegression(maxIter=10, regParam=0.01)\n\npipeline=Pipeline(stages[tokenizer, hashingTF, lr])\n\nmodel=pipeline.fit(training)\ntraining.show()\n","user":"anonymous","dateUpdated":"2018-03-14T19:22:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520919492395_517737356","id":"20180313-053812_1800594647","dateCreated":"2018-03-13T05:38:12+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"title":"Past 5 days dataset: Get the test dataset and store in local HDFS on Big Data Cloud","text":"%sh\n\ncurl -O https://raw.githubusercontent.com/sblack4/spark-streaming-ml-demo/master/data/testdata.csv\nhadoop fs -copyFromLocal -f testdata.csv /user/zeppelin/BigDataMLDemo/\nhadoop fs -chmod 777 /user/zeppelin/BigDataMLDemo/testdata.csv\nhadoop fs -ls /user/zeppelin/BigDataMLDemo/","user":"anonymous","dateUpdated":"2018-03-15T18:34:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":"false"},"editorMode":"ace/mode/sh","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r 71  246k   71  175k    0     0   122k      0  0:00:02  0:00:01  0:00:01  320k\r100  246k  100  246k    0     0   167k      0  0:00:01  0:00:01 --:--:--  420k\nFound 4 items\n-rwxrwxrwx   2 zeppelin hdfs       2397 2018-03-12 23:36 /user/zeppelin/BigDataMLDemo/Lat_Long_BuildingID.csv\n-rwxrwxrwx   2 zeppelin hdfs    3476701 2018-03-12 23:36 /user/zeppelin/BigDataMLDemo/building.csv\n-rwxrwxrwx   2 zeppelin hdfs    2762028 2018-03-13 01:50 /user/zeppelin/BigDataMLDemo/final_dataset.csv\n-rwxrwxrwx   2 zeppelin hdfs     252462 2018-03-15 16:13 /user/zeppelin/BigDataMLDemo/testdata.csv\n"}]},"apps":[],"jobName":"paragraph_1521053062219_1803415550","id":"20180314-184422_939754490","dateCreated":"2018-03-14T18:44:22+0000","dateStarted":"2018-03-15T16:13:06+0000","dateFinished":"2018-03-15T16:13:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"title":"Cleanse and Load the test dataset into SparkSQL Dataframe","text":"%pyspark\n\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark import sql\nfrom pyspark.sql import Row, SQLContext\n\nsqlContext=sql.SQLContext(sc)\n\nLabeledDocument = Row(\"RecordID\", \"SystemInfo\")\n\n# Define a function that parses the raw CSV file and returns an object of type LabeledDocument\n\ndef parseDocument(line):\n    values = [str(x) for x in line.split(',')]\n    textValue = str(values[1]) + \" \" + str(values[2])\n    return LabeledDocument((values[0]), textValue)\n\n# Load the raw test data file and parse it\ndata = sc.textFile(\"hdfs:///user/zeppelin/BigDataMLDemo/testdata.csv\")\n\ndocuments = data.map(parseDocument)\ntest = documents.toDF()\ntest.show(10)\n\n#df=sqlContext.read.format('csv').load('hdfs:///user/zeppelin/BigDataMLDemo/testdata.csv')\n#df.show(10)\n","user":"anonymous","dateUpdated":"2018-03-15T18:34:44+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 346, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 334, in <module>\n    exec(code)\n  File \"<stdin>\", line 12, in <module>\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 57, in toDF\n    return sparkSession.createDataFrame(self, schema, sampleRatio)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 524, in createDataFrame\n    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 364, in _createFromRDD\n    struct = self._inferSchema(rdd, samplingRatio)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/session.py\", line 335, in _inferSchema\n    first = rdd.first()\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1360, in first\n    rs = self.take(1)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/rdd.py\", line 1342, in take\n    res = self.context.runJob(self, takeUpToNumLeft, p)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 978, in runJob\n    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 67, in deco\n    e.java_exception.getStackTrace()))\n  File \"/u01/bdcsce/usr/local/lib/python2.7/_abcoll.py\", line 605, in __iter__\n    v = self[i]\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 191, in __getitem__\n    return self.__compute_item(key)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 167, in __compute_item\n    new_key = self.__compute_index(key)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 156, in __compute_index\n    size = len(self)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 239, in __len__\n    return get_return_value(answer, self._gateway_client)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 330, in get_return_value\n    if type == VOID_TYPE:\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_collections.py\", line 238, in __len__\n    answer = self._gateway_client.send_command(command)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 883, in send_command\n    response = connection.send_command(command)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1028, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n  File \"/u01/bdcsce/usr/local/lib/python2.7/socket.py\", line 451, in readline\n    data = self._sock.recv(self._rbufsize)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/context.py\", line 249, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"}]},"apps":[],"jobName":"paragraph_1521053255945_11314826","id":"20180314-184735_1041831344","dateCreated":"2018-03-14T18:47:35+0000","dateStarted":"2018-03-15T16:12:32+0000","dateFinished":"2018-03-15T16:12:34+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"title":"Store the test dataset into permanent Hive Table","text":"%pyspark\n\n\nfrom pyspark.sql import Hivecontext\nhivecontext=SQLContext(sC)\ntest.write.saveAsTable('test_data')\nresult=hivecontext.sql(\"SELECT * FROM test_data\")\nresult.show(10)\n\n","user":"anonymous","dateUpdated":"2018-03-15T18:34:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 346, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 334, in <module>\n    exec(code)\n  File \"<stdin>\", line 1, in <module>\nImportError: cannot import name Hivecontext\n\n"}]},"apps":[],"jobName":"paragraph_1521053334574_1670275462","id":"20180314-184854_906104106","dateCreated":"2018-03-14T18:48:54+0000","dateStarted":"2018-03-15T16:15:34+0000","dateFinished":"2018-03-15T16:15:34+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"title":"Apply Machine Learning Model on Test Dataset and Get the Predictions for Buildings that are at Risk of Damage","text":"%pyspark\n\n\nprediction = model.transform(test)\nselected = prediction.select(\"SystemInfo\", \"prediction\", \"probability\")\nfor row in selected.collect():\n    print row\n","user":"anonymous","dateUpdated":"2018-03-15T18:34:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true,"editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 346, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-9106667939756785646.py\", line 334, in <module>\n    exec(code)\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'model' is not defined\n\n"}]},"apps":[],"jobName":"paragraph_1521053471117_502930330","id":"20180314-185111_79598583","dateCreated":"2018-03-14T18:51:11+0000","dateStarted":"2018-03-15T16:15:38+0000","dateFinished":"2018-03-15T16:15:38+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"title":"Objective Achieved!!","text":"%md\nWohoo!!","user":"anonymous","dateUpdated":"2018-03-15T18:35:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Wohoo!!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521130538823_-1365908311","id":"20180315-161538_889816315","dateCreated":"2018-03-15T16:15:38+0000","dateStarted":"2018-03-15T18:35:38+0000","dateFinished":"2018-03-15T18:35:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"%md\n","user":"anonymous","dateUpdated":"2018-03-15T18:35:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":"true"},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521138938618_-296709804","id":"20180315-183538_2036946551","dateCreated":"2018-03-15T18:35:38+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:195"}],"name":"Skyline Luxury Properties: Predictive Analysis","id":"2DA5HJ1EA","angularObjects":{"2D83XJEP2:shared_process":[],"2D74Z2NB2:shared_process":[],"2DAPFFZWM:shared_process":[],"2D9JD68SJ:shared_process":[],"2DAP47QNG:shared_process":[],"2D8MNTGDS:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2DB12VEN7:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}